{
  "meta": {
    "project": "GhostType Backend",
    "date": "2026-02-20",
    "prepared_by": "Generated by TestSprite"
  },
  "product_overview": "Backend service for GhostType: a FastAPI server that exposes a health check HTTP endpoint and a streaming WebSocket /generate endpoint to perform multi-turn AI text generation with per-request model configuration, cancellation, and conversation resets.",
  "core_goals": [
    "Provide a low-latency streaming text-generation WebSocket for the macOS frontend to receive incremental tokens in real time.",
    "Expose a simple HTTP health check so the frontend can detect backend availability and auto-fallback to the stub agent.",
    "Support per-request model/provider configuration and env-var defaults so users can target different LLM providers and credentials from the frontend.",
    "Allow client-driven control: cancel in-progress generations and reset conversation history without reconnecting.",
    "Surface user-friendly errors and ensure safe threading/cancellation when bridging sync LLM callbacks to async WebSocket clients."
  ],
  "features": [
    {
      "name": "Health Check API",
      "description": "Simple HTTP GET endpoint returning server status and the currently configured provider/model info so the frontend can poll backend availability.",
      "user_flows": [
        "GET http://127.0.0.1:8420/health -> Receive 200 with JSON {\"status\":\"running\",\"provider\":\"<provider>\",\"model\":\"<model_id>\"}",
        "GET http://127.0.0.1:8420/health -> Receive 500 with error body or connection refused (backend down) indicating health check failure"
      ]
    },
    {
      "name": "Text Generation WebSocket",
      "description": "WebSocket streaming endpoint for multi-turn AI generation. Client opens ws://127.0.0.1:8420/generate, sends a prompt + context + mode + per-request config, and receives streamed token messages then a done/error/cancelled terminal message.",
      "user_flows": [
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"prompt\":\"...\",\"context\":\"...\",\"mode\":\"generate\",\"mode_type\":\"draft\",\"config\":{\"provider\":\"bedrock\",\"model_id\":\"...\",\"aws_profile\":\"...\",\"aws_region\":\"...\"}} -> Receive one or more messages of type=token with incremental content -> Receive message of type=done with the full response",
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send malformed JSON or message missing required 'prompt' -> Receive message of type=error with validation or parse error"
      ]
    },
    {
      "name": "Cancellation Support",
      "description": "Client can abort an in-progress streaming generation by sending a cancel message; the server will respond with a cancelled message and stop sending tokens.",
      "user_flows": [
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send generation request JSON {\"prompt\":\"...\",\"mode\":\"generate\", ...} -> Receive several messages of type=token -> Send JSON {\"type\":\"cancel\"} -> Receive message of type=cancelled",
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"type\":\"cancel\"} when no generation is active -> Receive message of type=error or no-op indicating there is no active generation to cancel"
      ]
    },
    {
      "name": "New Conversation Reset",
      "description": "Client can reset the agent's conversation history for the current WebSocket session by sending a new_conversation message; server acknowledges with a conversation_reset message.",
      "user_flows": [
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"type\":\"new_conversation\"} -> Receive message of type=conversation_reset confirming the reset",
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Close the WebSocket or lose connection -> Attempt to send JSON {\"type\":\"new_conversation\"} -> Client WebSocket send fails / raises error due to closed connection"
      ]
    },
    {
      "name": "Message Building",
      "description": "Server-side logic constructs the user/system messages based on request mode (generate, rewrite, fix, translate) and optional context/screenshot before passing them to the model; this is exercised via the /generate WebSocket request payload.",
      "user_flows": [
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"prompt\":\"Rewrite this sentence\",\"context\":\"previous turn text\",\"mode\":\"rewrite\",\"mode_type\":\"chat\",\"config\":{...}} -> Receive streaming token messages reflecting a rewrite -> Receive type=done with final rewritten text",
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"prompt\":\"Summarize\",\"mode\":\"summarize\"} (invalid/unsupported mode) -> Receive message of type=error indicating invalid mode"
      ]
    },
    {
      "name": "Model Configuration",
      "description": "Per-request ModelConfig is accepted in the WebSocket request and merged with backend env-var defaults; supports provider, model_id, aws_profile, aws_region, etc.",
      "user_flows": [
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"prompt\":\"...\",\"config\":{\"provider\":\"bedrock\",\"model_id\":\"<id>\",\"aws_profile\":\"...\",\"aws_region\":\"...\"}} -> Receive streaming token messages served by the selected bedrock model -> Receive type=done",
        "Open WebSocket connection to ws://127.0.0.1:8420/generate -> Send JSON {\"prompt\":\"...\",\"config\":{\"provider\":\"openai\",\"model_id\":\"gpt-...\"}} -> Receive message of type=error indicating provider not implemented (create_model raises ValueError for non-bedrock providers)"
      ]
    }
  ],
  "code_summary": {
    "version": "2",
    "type": "backend",
    "tech_stack": [
      "python",
      "fastapi",
      "websocket",
      "strands-agents-sdk",
      "uvicorn",
      "boto3"
    ],
    "features": [
      {
        "name": "Health Check API",
        "description": "Simple health check endpoint returning server status, provider, and model info",
        "files": [
          "ghosttype/backend/server.py",
          "ghosttype/backend/config.py"
        ],
        "endpoints": [
          {
            "method": "GET",
            "path": "/health",
            "description": "Returns server health status with provider and model information",
            "auth_required": false,
            "response_schema": {
              "200": {
                "status": "string",
                "provider": "string",
                "model": "string"
              }
            }
          }
        ],
        "depends_on": []
      },
      {
        "name": "Text Generation WebSocket",
        "description": "WebSocket endpoint for streaming AI text generation with multi-turn conversation support, cancellation, and per-request model configuration",
        "files": [
          "ghosttype/backend/server.py",
          "ghosttype/backend/agent.py",
          "ghosttype/backend/config.py"
        ],
        "endpoints": [
          {
            "method": "WEBSOCKET",
            "path": "/generate",
            "description": "Streaming text generation via WebSocket. Accepts prompt requests with context, mode, and config. Streams back tokens, done, error, or cancelled messages.",
            "auth_required": false,
            "request_schema": {
              "body": {
                "prompt": "string",
                "context": "string",
                "mode": "string (generate|rewrite|fix|translate)",
                "mode_type": "string (draft|chat)",
                "screenshot": "string (base64, optional)",
                "config": {
                  "provider": "string",
                  "model_id": "string",
                  "aws_profile": "string",
                  "aws_region": "string"
                }
              }
            },
            "response_schema": {
              "token": "type=token, content=partial text",
              "done": "type=done, content=full response",
              "error": "type=error, content=error message",
              "cancelled": "type=cancelled"
            }
          }
        ],
        "depends_on": []
      },
      {
        "name": "Cancellation Support",
        "description": "Client can send cancel messages during generation to abort streaming",
        "files": [
          "ghosttype/backend/server.py"
        ],
        "endpoints": [
          {
            "method": "WEBSOCKET",
            "path": "/generate",
            "description": "Send type=cancel message to abort an in-progress generation",
            "auth_required": false,
            "request_schema": {
              "body": {
                "type": "cancel"
              }
            },
            "response_schema": {
              "cancelled": "type=cancelled"
            }
          }
        ],
        "depends_on": [
          "Text Generation WebSocket"
        ]
      },
      {
        "name": "New Conversation Reset",
        "description": "Client can reset the conversation history to start fresh",
        "files": [
          "ghosttype/backend/server.py"
        ],
        "endpoints": [
          {
            "method": "WEBSOCKET",
            "path": "/generate",
            "description": "Send type=new_conversation message to reset agent state",
            "auth_required": false,
            "request_schema": {
              "body": {
                "type": "new_conversation"
              }
            },
            "response_schema": {
              "conversation_reset": "type=conversation_reset"
            }
          }
        ],
        "depends_on": [
          "Text Generation WebSocket"
        ]
      },
      {
        "name": "Message Building",
        "description": "Builds user messages based on mode (generate, rewrite, fix, translate) and optional context/screenshot",
        "files": [
          "ghosttype/backend/server.py"
        ],
        "endpoints": [],
        "depends_on": []
      },
      {
        "name": "Model Configuration",
        "description": "Per-request model configuration supporting multiple providers with env var defaults",
        "files": [
          "ghosttype/backend/agent.py",
          "ghosttype/backend/config.py"
        ],
        "endpoints": [],
        "depends_on": []
      }
    ],
    "known_limitations": [
      {
        "issue": "Only bedrock provider is implemented in create_model; other providers raise ValueError",
        "location": "ghosttype/backend/agent.py",
        "impact": "Requests with provider other than bedrock will fail"
      },
      {
        "issue": "CORS allows all origins",
        "location": "ghosttype/backend/server.py",
        "impact": "No origin restriction on API access"
      }
    ]
  }
}
